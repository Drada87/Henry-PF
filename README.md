# Rebuilding the Pistons ğŸ€

**StoryTelling** â€“ Un proyecto de anÃ¡lisis deportivo enfocado en la reconstrucciÃ³n de los Detroit Pistons.

---

## ğŸ‘¥ Equipo

**StoryTelling Data Analytics** 

- David Rada  
- IvÃ¡n HernÃ¡ndez  
- Manuel SÃ¡nchez

---

## ğŸ¯ Objetivo del anÃ¡lisis

El propÃ³sito principal de este proyecto es entender las causas del bajo rendimiento de los Detroit Pistons en la Ãºltima dÃ©cada, y proponer recomendaciones a travÃ©s de un anÃ¡lisis integral de datos.  
Nos enfocamos en detectar oportunidades de mejora a partir del anÃ¡lisis de jugadores, salarios, picks del draft y eficiencia econÃ³mica y deportiva.

---

## ğŸ›  TecnologÃ­as utilizadas

- Google Cloud SQL (SQLserver)
- DBeaver
- Python  
  - Pandas  
  - NumPy   
- Jupyter Notebook  
- GitHub

---

## âš™ï¸ MetodologÃ­a aplicada en Sprint 1

- DiseÃ±o de modelo entidad-relaciÃ³n  
- CreaciÃ³n de instancia en Google Cloud  
- Desarrollo de scripts SQL para la creaciÃ³n de tablas  
- AnÃ¡lisis exploratorio inicial con Pandas  
- AutomatizaciÃ³n de scraping para datos de salarios y MVPs  
- ConfiguraciÃ³n del entorno colaborativo en GitHub

---

## ğŸ“Š Datos utilizados

**Fuentes de datos:**

- Kaggle (NBA Dataset)
- HoopsHype (Salarios â€“ scraping personalizado)
- Basketball Reference (MVPs â€“ scraping personalizado)

**Contenido:**

- EstadÃ­sticas individuales
- InformaciÃ³n de drafts
- Salarios histÃ³ricos
- Premios MVP
- MÃ©tricas colectivas por equipo

---

## ğŸ§© Modelo entidad-relaciÃ³n

**Tablas diseÃ±adas:**

- `players`  
- `salaries`  
- `draft`  
- `mvps`  
- `teams`  

Incluye claves primarias y forÃ¡neas, tipos de datos definidos y relaciones 1:N entre jugadores y equipos.

---

## ğŸ“ Estructura del repositorio

```
EDA/     â†’ notebooks de anÃ¡lisis exploratorio  
ETL/     â†’ scripts de limpieza y scraping  
SQL/     â†’ scripts de creaciÃ³n de base y tablas  
DOCS/    â†’ presentaciones y documentaciÃ³n adicional
```

---

## âœ… Estado actual - Cierre del Sprint 1

- Repositorio configurado y compartido  
- Datos recopilados y limpiados  
- Base de datos en la nube funcional  
- ConexiÃ³n verificada desde Python 
- Proceso de scraping automatizable validado  
- Estructura de trabajo colaborativa establecida
